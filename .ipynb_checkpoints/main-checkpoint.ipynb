{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZWlN-YIa0sc"
      },
      "source": [
        "# **BASE DE DADOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6t4DttXecxX"
      },
      "source": [
        "- Instalação e Importações\n",
        "\n",
        "  Execute esta células abaixo primeiro para garantir que as bibliotecas estejam presentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU4uY3mJa44E"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "import networkx as nx\n",
        "import wikipedia\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eEQRIOoha72X"
      },
      "outputs": [],
      "source": [
        "# Configurando para a Wikipédia em Português (essencial para seus seeds)\n",
        "wikipedia.set_lang(\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS0N0yLOee0Z"
      },
      "source": [
        "- Configuração dos Seeds e Stopwords\n",
        "\n",
        "  Aqui definimos os 5 temas e as palavras de parada para evitar páginas administrativas (como ISBN, Categorias, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB4shAwGempz",
        "outputId": "10a98ed6-58c3-4c4b-92c1-42d7f3fdac03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeds configurados: ['Transformer (modelo de aprendizado de máquina)', 'The Beatles', 'Revolução Francesa', 'One Piece', 'Rio Grande do Norte']\n"
          ]
        }
      ],
      "source": [
        "# Definição dos SEEDS (Pontos de partida)\n",
        "SEEDS = [\n",
        "    \"Transformer (modelo de aprendizado de máquina)\", # Ajustado para o título correto na PT-Wiki\n",
        "    \"The Beatles\",\n",
        "    \"Revolução Francesa\",\n",
        "    \"One Piece\",\n",
        "    \"Rio Grande do Norte\"\n",
        "]\n",
        "\n",
        "# Lista de Stopwords\n",
        "STOPS = (\n",
        "    \"International Standard Serial Number\",\n",
        "    \"International Standard Book Number\",\n",
        "    \"National Diet Library\",\n",
        "    \"International Standard Name Identifier\",\n",
        "    \"Digital Object Identifier\",\n",
        "    \"Arxiv\",\n",
        "    \"PubMed\",\n",
        "    \"Bibcode\",\n",
        "    \"Jstor\",\n",
        "    \"Doi (Identificador)\",\n",
        "    \"Isbn (Identificador)\",\n",
        "    \"Pmid (Identificador)\",\n",
        "    \"Arxiv (Identificador)\",\n",
        "    # Adições específicas para limpar a Wikipédia em PT\n",
        "    \"Wikipédia\",\n",
        "    \"Ajuda\",\n",
        "    \"Ficheiro\",\n",
        "    \"Categoria\",\n",
        "    \"Portal\",\n",
        "    \"Especial\",\n",
        "    \"Livro\",\n",
        "    \"Predefinição\"\n",
        ")\n",
        "\n",
        "print(f\"Seeds configurados: {SEEDS}\")\n",
        "\n",
        "TEMAS_CHAVE = [\n",
        "    \"aprendizado\", \"neural\", \"inteligência\",\n",
        "    \"rock\", \"música\", \"banda\",\n",
        "    \"história\", \"século\",\n",
        "    \"anime\", \"mangá\", \"luffy\",\n",
        "    \"nordeste\", \"brasil\", \"natal\"\n",
        "    \"beatles\", \"transform\", \"revolu\",\n",
        "    \"franç\", \"piece\", \"norte\",\n",
        "]\n",
        "\n",
        "CHECKPOINT_INTERVAL = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEt8RaCke7g7"
      },
      "source": [
        "- Construção da Rede\n",
        "  \n",
        "  Este bloco constitui o núcleo da coleta de dados. A implementação clássica do algoritmo de Busca em Largura (Breadth-First Search - BFS) foi adaptada para suportar múltiplos seeds e atingir a profundidade alvo de nível 2 (altura < 3).\n",
        "\n",
        "  Conforme destacado no Requisito 4, a expansão da rede até este nível acarreta um crescimento exponencial no número de nós e arestas, gerando uma demanda computacional proibitiva para uma busca exaustiva simples. Para viabilizar a coleta e manter o foco nos temas de interesse, substituímos a abordagem padrão por uma estratégia de Busca Baseada em Heurística.\n",
        "\n",
        "### Metodologia Aplicada:\n",
        "\n",
        "1.  **Estrutura de Dados (Priority Queue):**\n",
        "    Substituímos a estrutura de fila convencional (FIFO), típica do BFS, por uma Fila de Prioridade implementada sobre um Min-Heap (heapq). Essa alteração permite que o algoritmo abandone o processamento sequencial cego em favor de uma abordagem ordenada pelo \"custo\", onde nós de maior relevância (menor score) são processados primeiro.\n",
        "\n",
        "2.  **Heurística de Relevância (Score):**\n",
        "    Implementamos uma função de custo `calcular_heuristica` que atribui prioridade aos links baseando-se em:\n",
        "    * **Bonificação (-50 pontos):** Se o título da página contiver palavras-chave relacionadas aos 5 SEEDs originais (ex: \"Beatles\", \"Revolução\", \"Brasil\"). Isso mantém a coesão temática da rede.\n",
        "    * **Penalidade (+20 pontos):** Se o título for excessivamente longo (> 40 caracteres), indicando ser um tópico muito específico ou administrativo.\n",
        "    * **Ordem de Camada:** A profundidade (layer) continua sendo o critério primário de ordenação para garantir a expansão uniforme até o nível desejado.\n",
        "\n",
        "Dessa forma, garantimos a construção de uma base de dados rica e que resolve o nosso problema do altura <3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mPkVPa0cflNg",
        "outputId": "3f01d360-f7dd-4540-c085-d276ff4c084b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeds configurados: ['Transformer (modelo de aprendizado de máquina)', 'The Beatles', 'Revolução Francesa', 'One Piece', 'Rio Grande do Norte']\n",
            "Iniciando coleta SEGURA (com Checkpoints)...\n",
            "Camada 0 | Score 0 | Processando: One Piece | Grafo: 0 nós\n",
            "Camada 0 | Score 0 | Processando: Revolução Francesa | Grafo: 309 nós\n",
            "Camada 0 | Score 0 | Processando: Rio Grande do Norte | Grafo: 935 nós\n",
            "Camada 0 | Score 0 | Processando: The Beatles | Grafo: 1694 nós\n",
            "Camada 0 | Score 0 | Processando: Transformer (modelo de aprendizado de máquina) | Grafo: 3369 nós\n",
            "Camada 1 | Score 50 | Processando: Distrito Federal (Brasil) | Grafo: 10548 nós\n"
          ]
        }
      ],
      "source": [
        "def calcular_heuristica(titulo_link, layer):\n",
        "    \"\"\"\n",
        "    Define a prioridade (Menor score = Maior prioridade).\n",
        "    \"\"\"\n",
        "    score = 100 # Base\n",
        "    titulo_lower = titulo_link.lower()\n",
        "\n",
        "    # Bonificação: Se o link tem a ver com seus temas, score diminui\n",
        "    for termo in TEMAS_CHAVE:\n",
        "        if termo in titulo_lower:\n",
        "            score -= 50\n",
        "            break # Aplica bônus apenas uma vez\n",
        "\n",
        "    # Penalidade: Títulos muito longos ou muito curtos\n",
        "    if len(titulo_link) > 50 or len(titulo_link) < 3:\n",
        "        score += 20\n",
        "\n",
        "    # Retorna tupla para o Min-Heap: (Layer, Score, Título)\n",
        "    return (layer, score, titulo_link)\n",
        "\n",
        "# --- INICIALIZAÇÃO ---\n",
        "priority_queue = []\n",
        "\n",
        "# Adiciona as Seeds iniciais na fila\n",
        "for seed in SEEDS:\n",
        "    heapq.heappush(priority_queue, (0, 0, seed))\n",
        "\n",
        "todo_set = set(SEEDS)\n",
        "done_set = set()\n",
        "g = nx.DiGraph()\n",
        "\n",
        "print(f\"Seeds configurados: {SEEDS}\")\n",
        "print(\"Iniciando coleta SEGURA (com Checkpoints)...\")\n",
        "\n",
        "# --- LOOP PRINCIPAL (COM PROTEÇÃO CONTRA FALHAS) ---\n",
        "try:\n",
        "    while priority_queue:\n",
        "        # Pega o item de maior prioridade\n",
        "        layer, score, page = heapq.heappop(priority_queue)\n",
        "\n",
        "        # Regra de parada: Altura < 3 (Processa camadas 0, 1 e 2)\n",
        "        if layer >= 3:\n",
        "            continue\n",
        "\n",
        "        if page in done_set:\n",
        "            continue\n",
        "\n",
        "        done_set.add(page)\n",
        "\n",
        "        # --- CHECKPOINT AUTOMÁTICO ---\n",
        "        if len(done_set) % CHECKPOINT_INTERVAL == 0:\n",
        "            print(f\"--- [Auto-Save] Salvando checkpoint com {len(done_set)} nós... ---\")\n",
        "            nx.write_graphml(g, \"checkpoint_parcial.graphml\")\n",
        "\n",
        "        # Feedback visual\n",
        "        if len(done_set) % 50 == 0 or layer == 0:\n",
        "            print(f\"Camada {layer} | Score {score} | Processando: {page} | Grafo: {len(g)} nós\")\n",
        "\n",
        "        try:\n",
        "            # Tenta baixar a página\n",
        "            wiki = wikipedia.page(page, auto_suggest=False)\n",
        "\n",
        "            for link in wiki.links:\n",
        "                # Limpeza simples\n",
        "                link_clean = link.strip()\n",
        "\n",
        "                # Verifica se NÃO é stopword e não começa com prefixos indesejados\n",
        "                # Verifica também se não contém substrings proibidas (ex: \"Ficheiro:\")\n",
        "                if (link_clean not in STOPS) and \\\n",
        "                   (not link_clean.startswith(\"Lista de\")) and \\\n",
        "                   (\":\" not in link_clean): # Remove namespaces como Categoria:..., Ficheiro:...\n",
        "\n",
        "                    g.add_edge(page, link_clean)\n",
        "\n",
        "                    if link_clean not in todo_set and link_clean not in done_set:\n",
        "                        # Calcula a heurística para o novo link\n",
        "                        prioridade = calcular_heuristica(link_clean, layer + 1)\n",
        "\n",
        "                        heapq.heappush(priority_queue, prioridade)\n",
        "                        todo_set.add(link_clean)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n!!! Interrupção manual detectada pelo usuário !!!\")\n",
        "            raise # Joga para o 'finally' salvar\n",
        "        except Exception:\n",
        "            # Erros de página (PageError, Disambiguation) são ignorados para não parar o fluxo\n",
        "            continue\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERRO INESPERADO: {e}\")\n",
        "\n",
        "finally:\n",
        "    # --- SALVAMENTO FINAL DE SEGURANÇA ---\n",
        "    print(\"-\" * 30)\n",
        "    print(\"ENCERRANDO EXECUÇÃO...\")\n",
        "    arquivo_final = \"grafo_resgate_emergencia.graphml\"\n",
        "    nx.write_graphml(g, arquivo_final)\n",
        "    print(f\"Salvamento de emergência concluído em: '{arquivo_final}'\")\n",
        "    print(f\"Total coletado: {len(g)} nós e {g.number_of_edges()} arestas.\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVaS9W8-fvMW"
      },
      "source": [
        "- Tratamento de Dados (Limpeza)\n",
        "  Remoção de duplicatas (ex: plurais) e auto-loops, conforme feito no notebook do professor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHa--LeEf3im"
      },
      "outputs": [],
      "source": [
        "print(f\"Iniciando limpeza em grafo de {len(g)} nós...\")\n",
        "print(\"Isso pode demorar um pouco devido ao tamanho da rede.\")\n",
        "\n",
        "# 1. Remover auto-loops (Rápido e seguro)\n",
        "g.remove_edges_from(nx.selfloop_edges(g))\n",
        "\n",
        "# 2. Fundir plurais (Ex: 'Palavra' e 'Palavras') com verificação\n",
        "duplicates = [(node, node + \"s\") for node in g if node + \"s\" in g]\n",
        "for u, v in duplicates:\n",
        "    if u in g and v in g: # <--- PROTEÇÃO: Só funde se ambos ainda existirem\n",
        "        g = nx.contracted_nodes(g, u, v, self_loops=False)\n",
        "\n",
        "# 3. Fundir hífens (Ex: 'One-Punch Man' e 'One Punch Man') com verificação\n",
        "duplicates_hyphen = [(node, node.replace(\"-\", \" \")) for node in g\n",
        "                     if node.replace(\"-\", \" \") in g and node != node.replace(\"-\", \" \")]\n",
        "\n",
        "for u, v in duplicates_hyphen:\n",
        "    if u in g and v in g: # <--- PROTEÇÃO: Só funde se ambos ainda existirem\n",
        "        g = nx.contracted_nodes(g, u, v, self_loops=False)\n",
        "\n",
        "# Limpeza técnica de atributos (para evitar erros no Gephi)\n",
        "for node in g.nodes():\n",
        "    g.nodes[node].pop('contraction', None)\n",
        "for u, v in g.edges():\n",
        "    g.edges[u, v].pop('contraction', None)\n",
        "\n",
        "print(f\"Limpeza concluída! Grafo atual: {len(g)} nós.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBRPrZI0f7Y_"
      },
      "source": [
        "- Filtragem e Exportação\n",
        "  \n",
        "  Gera o arquivo final .graphml."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbgLT5Vbf_uQ"
      },
      "outputs": [],
      "source": [
        "# Filtra o \"Core\" da rede: Mantém apenas nós com grau >= 2\n",
        "# (Nós que têm pelo menos 2 conexões de entrada ou saída)\n",
        "core = [node for node, deg in dict(g.degree()).items() if deg >= 2]\n",
        "gsub = g.subgraph(core).copy()\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Grafo Original: {len(g)} nós\")\n",
        "print(f\"Grafo Final (Core): {len(gsub)} nós\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Salva o arquivo para o Gephi\n",
        "nome_arquivo = \"trabalho_final_validacao.graphml\"\n",
        "nx.write_graphml(gsub, nome_arquivo)\n",
        "\n",
        "print(f\"Arquivo '{nome_arquivo}' salvo com sucesso!\")\n",
        "print(\"Faça o download na aba de arquivos (ícone da pasta) à esquerda.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
